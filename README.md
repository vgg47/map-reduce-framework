# ComputeGraph

Библиотека для вычислений над графами в парадигме MapReduce.

Графы работают в стриминговой манере, никакая часть графа вычислений не накапливает
потенциально неограниченный набор значений в оперативной памяти;  потребление памяти
константно вне зависимости от числа записей во входных потоках.

Написанное выше верно по модулю сортировки, которая выполняется в соседнем процессе.


## Интерфейс графа вычислений

Граф вычислений состоит из точек входа для данных и операций над ними. И точки входа, и операции являются равноправными узлами графа.

Над входными данным запускаются операции.
В общем случае операции вызываются последовательно, но есть операции (`join`), которые на вход принимают другие
графы вычислений; за счет этого полный граф вычислений может быть нелинеен.


Для создания графа существует два метода, оба вызываются у класса `Graph`:
* ```graph_from_iter``` принимает либо объект класса `Graph` и создает его копию, либо
строку, которая является ключом в словаре источников (об этом ниже).
* ```graph_from_file``` принимает путь до файла и парсер. В момент начала вычислений построчно читает файл и передает их парсеру. Результат парсера передается дальше по графу.

Каждая операция в графе задается вызовом соответствующего метода класса `Graph`. Все операции содержатся в файле `lib/operations.py`.

Существует четыре операции, функциональность которых соответствует парадигме MapReduce: 
* `Map` принимает операцию типа ```Mapper```
* `Reduce`принимает операцию типа `Reducer` и лист строк с ключами, по которым производить reduce.
* `Sort` принимает лист строк с ключами, по которым производить сортировку.
* `Join` принимает операцию типа  `Joiner`, второй граф и лист строк, по которым выполнять join.

Пример графа, который подсчитывает кол-во слов в документах:
```python
graph = Graph.graph_from_iter('texts') \
    .map(operations.FilterPunctuation('text')) \
    .map(operations.LowerCase('text')) \
    .map(operations.Split('text')) \
    .sort(['text']) \
    .reduce(operations.Count('count'), ['text']) \
    .sort(['count', 'text'])
```

Заострю внимание: в момент создания графа не производится никаких чтений данных и вычислений.

Входные данные могут подаваться как в виде имен файлов. Обратите внимание, что генераторы необходимо подавать на вход графу в виде **фабрик**,
то есть в виде объектов, которые надо позвать, чтобы получить итератор. 

Пример запуска вычислений в графе:
```python
dummy_graph = Graph.graph_from_iter(name='docs')

graph.run(docs=lambda: iter([{'key': 'value'}]))

```

Однажды созданный граф можно запускать на разных входах без пересоздания.

## Примеры 

В файле `examples.py` лежат два примера использования библиотеки.
В нем уже описанным в файле `graphs.py` графам передаются источники данных, их структура и структура вывода. После этого запускаются вычисления и результаты печатаются с stdout.


## Задачи

Помимо примеров использования в репозитории имплементированы 4 графа, который можно использовать "из коробки". Реализации находятся в `graphs.py`.

#### Word Count

Граф принимает таблицу со строками в формате `{'doc_id': ..., 'text :...'}`.
Для каждого из слов, встречающихся в колонке `text`, считает количество вхождений во всю таблицу в сумме.

#### Инвертированный индекс на tf-idf

Работает с той же таблицей, что и в Word Count.

Для каждой пары (слово, документ) tf-idf считает так:
```
TFIDF(word_i, doc_i) = (frequency of word_i in doc_i) * log((total number of docs) / (docs where word_i is present))
```

Результатом является топ-3 документов по tf-idf для каждого слова.

#### Топ слов с наибольшей взаимной информацией

Задача, обратная предыдущей: для каждого документа считает топ-10 слов, наиболее характерных для него.

Ранжирует слова по метрике
[Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information).

Более формально задача ставится так: для каждого текста надо найти топ-10 слов, каждое из которых длиннее четырех
символов и встречается в каждом из документов не менее двух раз; топ надо выбирать по величине:
```
pmi(word_i, doc_i) = log((frequency of word_i in doc_i) / (frequency of word_i in all documents combined))
```

#### Средняя скорость движения по городу от часа и дня недели

Данные граф работает с информацией о движении людей на машинах по какому-то подмножеству улиц города Москвы.

Улицы города заданы как граф, а информация о передвижении задана как таблица, в каждой строке которой данные вида
```
{'edge_id': '624', 'enter_time': '20170912T123410.1794', 'leave_time': '20170912T123412.68'}
```
где `edge_id` — идентификатор ребра графа дорог (то есть просто участка какой-то улицы), а `enter_time` и `leave_time` —
соответственно время въезда и выезда с/на это ребро (время в UTC).

Также необходимо передать вспомогательную таблица вида
```
{'edge_id': '313', 'length':121, 'start': [37.31245, 51.256734], 'end': [37.31245, 51.256734]}
```
где `length` - длина в метрах, `start` и `end` — координаты начала и конца ребра, заданные в формате `('lon', 'lat')`.

Результатом работы является таблица со средней скоростью движения по городу в км/ч
в зависимости от часа и дня недели:
```
{'weekday': 'Mon', 'hour': 4, 'speed': 44.812}
```


## Тесты

Тесты на операции находятся в файле `lib/test_public.py`. Тесты проверяющие корректность работы имплементированных графов в `test_public.py`.

Для запуска тестов необходимо позвать pytest из корневой папки проекта.
